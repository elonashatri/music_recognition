{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'/data/home/acw507/.conda/envs/pytorchenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fd7d1b7caa39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# path to the full dataset:  /import/c4dm-05/elona/Corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'/data/home/acw507/.conda/envs/pytorchenv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/share/apps/centos7/anaconda3/2020.02/lib/python3.7/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '/data/home/acw507/.conda/envs/pytorchenv'"
     ]
    }
   ],
   "source": [
    "#paths\n",
    "\n",
    "# path to the half of the dataset:  /import/c4dm-05/elona/Capitan\n",
    "# path to the full dataset:  /import/c4dm-05/elona/Corpus\n",
    "import os\n",
    "print (os.environ['/data/home/acw507/.conda/envs/pytorchenv'])\n",
    "whole_primus = '/data/scratch/acw507/Corpus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the set of data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4dc270272c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import glob\n",
    "\n",
    "def loadPrimus(path, encoding, output_file):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    limit = 80000\n",
    "    i = 0 \n",
    "    with open(output_file, \"a\") as f: # \"a\" is for appending instead of writing\n",
    "        for folder in os.listdir(path):\n",
    "            # get the image file name\n",
    "            img_filename = f\"{folder}.png\"\n",
    "            img_path = os.path.join(path, folder, img_filename)\n",
    "            img = cv2.imread(img_path, 0)\n",
    "            X.append(img)\n",
    "            # print(img_path)\n",
    "\n",
    "            # get the corresponding agnostic file name\n",
    "            agnostic_filename = f\"{folder}.agnostic\"\n",
    "            agnostic_file_path = os.path.join(path, folder, agnostic_filename)\n",
    "\n",
    "            # read the contents of the agnostic file\n",
    "            with open(agnostic_file_path, \"r\") as af:\n",
    "                agnostic_contents = af.read()\n",
    "                if encoding == \"standard\":\n",
    "                    Y.append(agnostic_contents.strip().split(\"\\t\"))\n",
    "                else:\n",
    "                    splitted_seq = []\n",
    "                    sequence = agnostic_contents.strip().split(\"\\t\")\n",
    "                    for element in sequence:\n",
    "                        for char in element.split(\"-\"):\n",
    "                            splitted_seq.append(char)\n",
    "                    Y.append(splitted_seq)\n",
    "\n",
    "            # write the image filename and the agnostic file contents to the output file\n",
    "            f.write(f\"{img_filename}\\t\")\n",
    "            f.write(f\"{agnostic_contents}\\n\")\n",
    "\n",
    "            if i > limit:\n",
    "                break\n",
    "            i+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loadPrimus('/data/scratch/acw507/Corpus/', 'agnostic', '/data/home/acw507/music_recognition/data/only_shape/primus_whole_GT.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE A FILE WITH UNIQUE CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just for the sake of having a file that lists all possible classes with position\n",
    "\n",
    "import re \n",
    "\n",
    "def get_unique_classes(file_path):\n",
    "    # Open the file and read in the contents\n",
    "    with open(file_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "        contents = re.sub(r'\\S+\\.png', '', contents)\n",
    "    # Split the contents into words\n",
    "   \n",
    "    classes = contents.split()\n",
    "    # Create a set of unique words\n",
    "    unique_classes = set(classes)\n",
    "    # remove number_only_words from unique_words\n",
    "    # unique_words = [word for word in unique_words if not word.isnumeric()]\n",
    "    unique_classes = sorted(unique_classes)\n",
    "    # Write the unique words to a new .txt file\n",
    "    with open('/homes/es314/deep-text-recognition-benchmark/new_primus_clean/unqiue_classes.txt', 'w') as outfile:\n",
    "        for word in unique_classes:\n",
    "            # remove the last three characters of the class\n",
    "            # word = word[:-3]\n",
    "            outfile.write(word + '\\n')\n",
    "    return unique_classes\n",
    "\n",
    "\n",
    "unique_classes = get_unique_classes('/homes/es314/deep-text-recognition-benchmark/new_primus_clean/primus_whole_GT.txt')\n",
    "\n",
    "# print(unique_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN OUT POSITION OF NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(file_path):\n",
    "    # Open the file and read in the contents\n",
    "    with open(file_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "        \n",
    "    # Split the contents into words\n",
    "    words = contents.split()\n",
    "    with open('/homes/es314/deep-text-recognition-benchmark/new_primus_clean/whole_clean_1.txt', 'w') as outfile:\n",
    "        for word in words:\n",
    "            if '/' in word:\n",
    "                word = word.replace('/', '')\n",
    "            if word.endswith(\".png\"):\n",
    "                outfile.write('\\n')\n",
    "                word=word\n",
    "            else:\n",
    "                word = word[:-3]\n",
    "                if '- ' in word:\n",
    "                    word = word.replace('- ', ' ')\n",
    "            outfile.write(word + ' ')\n",
    "\n",
    "    return words\n",
    "\n",
    "words = get_words('/homes/es314/deep-text-recognition-benchmark/new_primus_clean/whole_primus_GT.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce another file with unique classes without positions\n",
    "\n",
    "# this is just for the sake of having a file that lists all possible classes with position\n",
    "\n",
    "import re \n",
    "\n",
    "def get_unique_classes(file_path):\n",
    "    # Open the file and read in the contents\n",
    "    with open(file_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "        contents = re.sub(r'\\S+\\.png', '', contents)\n",
    "    # Split the contents into words\n",
    "   \n",
    "    classes = contents.split()\n",
    "    # Create a set of unique words\n",
    "    unique_classes = set(classes)\n",
    "    # remove number_only_words from unique_words\n",
    "    # unique_words = [word for word in unique_words if not word.isnumeric()]\n",
    "    unique_classes = sorted(unique_classes)\n",
    "    # Write the unique words to a new .txt file\n",
    "    with open('/homes/es314/deep-text-recognition-benchmark/new_primus_clean/no_position_unique_classes.txt', 'w') as outfile:\n",
    "        for word in unique_classes:\n",
    "            # remove the last three characters of the class\n",
    "            # word = word[:-3]\n",
    "            outfile.write(word + '\\n')\n",
    "    return unique_classes\n",
    "\n",
    "\n",
    "unique_classes = get_unique_classes('/homes/es314/deep-text-recognition-benchmark/new_primus_clean/whole_clean_1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE MAPPING DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the mapping from json file\n",
    "with open(\"/homes/es314/deep-text-recognition-benchmark/new_primus_clean/mapping.json\") as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "# Read the contents of test.txt\n",
    "with open(\"/homes/es314/deep-text-recognition-benchmark/new_primus_clean/whole_clean_1.txt\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Replace each symbol with the corresponding mapping\n",
    "for symbol, replacement in mapping.items():\n",
    "    content = content.replace(symbol, replacement)\n",
    "\n",
    "# Write the modified content to a new file\n",
    "with open(\"primus_half_mapped.txt\", \"w\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_252398/1929198776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# write the dictionary to output file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# with open(\"output.txt\", \"w\") as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/bttr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "# the dict\n",
    "\n",
    "import json\n",
    "\n",
    "# read the input file\n",
    "with open(\"/homes/es314/deep-text-recognition-benchmark/new_primus_clean/no_position_unique_classes_whole.txt\", \"r\") as f:\n",
    "    values = f.read().split(\"\\n\")\n",
    "\n",
    "# create the dictionary\n",
    "encoding = {}\n",
    "characters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!#$%&\"\n",
    "for i, value in enumerate(values):\n",
    "    encoding[value] = characters[i % len(characters)]\n",
    "\n",
    "# write the dictionary to output file\n",
    "# with open(\"output.txt\", \"w\") as f:\n",
    "    json.dump(encoding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "22bf943db7a4016e127f44a22072fb1ca4d1173d810fc6bc6f779086a33510c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
